Um problema fundamental ao construir aplicativos voltados para o cliente é que os usuários de hoje têm baixa tolerância para tempos de carregamento lentos.

A questão da latência torna-se ainda mais crítica quando os clientes empresariais querem usar a GenAI para aumentar a produtividade pessoal ou construir produtos com base em um modelo de IA. Nesses cenários, as apostas são maiores porque os atrasos frustram os usuários e impactam diretamente as operações de negócios e as receitas. 

Em relação à latência, o maior desafio dos LLMs atualmente é que os modelos usam uma arquitetura auto-regressiva onde cada palavra gerada depende das palavras que vieram antes dele. Esta natureza sequencial limita a velocidade em que as saídas podem ser produzidas porque cada passo deve esperar que a anterior seja concluída. Se um LLM gera sequencialmente cada palavra na frase: "Meu esporte favorito é basquete". Em 0,2 segundos por palavra, levaria um segundo para completar a frase. Para resolver esse problema, desenvolvedores de IA estão explorando diferentes tipos de arquitetura e computação paralela.

Uma estratégia imediata eficaz é otimizar o tamanho do modelo. 
Modelos menores são frequentemente mais rápidos do que seus pares maiores e mais complexos.