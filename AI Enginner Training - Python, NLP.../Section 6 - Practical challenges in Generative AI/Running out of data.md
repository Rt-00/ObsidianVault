
É natural esperar que a próxima versão de um LLM seja significativamente maior do que a anterior. Mas é provável que uma questão importante impeça este processo. 

Se o GPT-4 já tiver lido e processado uma parte significativa dos dados publicamente disponíveis na internet, de onde viriam os novos dados para o GPT-5? Os desenvolvedores de IA correm o risco de ficar sem dados e enfrentam um possível abrandamento no crescimento das capacidades do modelo.

Além disso, agora que o ChatGPT ganhou popularidade, há um risco significativo de que muitos dos novos artigos e materiais postados on-line serão escritos usando IA. Isso torna o aprendizado de informações novas ou nuances desafiador para futuros LLMs porque eles frequentemente encontram conteúdo repetitivo ou derivado. 

Modelos futuros simplesmente ecoam o que modelos anteriores escreveram, aumentando assim os riscos de alucinações, vieses e imprecisão. Outro obstáculo à disponibilidade de dados é que, seguindo o hype em torno da Gen AI, várias grandes organizações em todo o mundo apresentaram ações judiciais e proibiram o scrapping de dados de seus sites.

Alguns dos primeiros problemas vieram do New York Times, Shutterstock, e do autor mais vendido John Grisham, enquanto plataformas como Reddit e Quora mudaram suas políticas para tornar ilegal para desenvolvedores de IA como OpenAI fazer o scrap de seus dados de plataforma.

Para mitigar esse problema, a OpenAI iniciou um programa de licenciamento de conteúdo e já assinou acordos com várias organizações que possuem alguns dos maiores conjuntos de dados proprietários do mundo. A OpenAI ofereceu aos editores entre 1.000.000.000 de dólares por ano para acessar arquivos e treinar seus modelos de IA Gen. Eles assinaram acordos com organizações como Shutterstock, Axel Springer, a Associated Press, The Monde e Pré-Semitia. 

No futuro, a concorrência entre grandes empresas de tecnologia que desenvolvem LLMs provavelmente aumentará significativamente o preço de grandes conjuntos de dados proprietários.